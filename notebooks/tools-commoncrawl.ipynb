{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Straylight\n",
    "\n",
    "### Common Crawl Domain Reconnaissance\n",
    "\n",
    "__Introduction:__\n",
    "This notebook will provide the ability to configure and search the publicly available Common Crawl dataset of websites. Common Crawl is a freely available dataset which contains over 8 years of crawled data including over 25 billion websites, trillions of links, and petabytes of data.\n",
    "\n",
    "__GitHub:__\n",
    "* https://github.com/brevityinmotion/straylight\n",
    "\n",
    "__Blog:__\n",
    "* [Search the html across 25 billion websites for passive reconnaissance using common crawl](https://medium.com/@brevityinmotion/search-the-html-across-25-billion-websites-for-passive-reconnaissance-using-common-crawl-7fe109250b83?sk=5b8b4a7c506d5acba572c0b30137f7aa)\n",
    "\n",
    "___Credits:___\n",
    "* Special thank you to Sebastian Nagel for the tutorials and insight for utilizing the dataset!\n",
    "* Many of the functions and code have been adapted from: http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior to utilizing this notebook, the following three queries should be run within AWS Athena to configure the Common Crawl database.\n",
    "\n",
    "#### Query 1\n",
    "<code>CREATE DATABASE ccindex</code>\n",
    "\n",
    "#### Query 2\n",
    "<code>CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (\n",
    "  url_surtkey                   STRING,\n",
    "  url                           STRING,\n",
    "  url_host_name                 STRING,\n",
    "  url_host_tld                  STRING,\n",
    "  url_host_2nd_last_part        STRING,\n",
    "  url_host_3rd_last_part        STRING,\n",
    "  url_host_4th_last_part        STRING,\n",
    "  url_host_5th_last_part        STRING,\n",
    "  url_host_registry_suffix      STRING,\n",
    "  url_host_registered_domain    STRING,\n",
    "  url_host_private_suffix       STRING,\n",
    "  url_host_private_domain       STRING,\n",
    "  url_protocol                  STRING,\n",
    "  url_port                      INT,\n",
    "  url_path                      STRING,\n",
    "  url_query                     STRING,\n",
    "  fetch_time                    TIMESTAMP,\n",
    "  fetch_status                  SMALLINT,\n",
    "  content_digest                STRING,\n",
    "  content_mime_type             STRING,\n",
    "  content_mime_detected         STRING,\n",
    "  content_charset               STRING,\n",
    "  content_languages             STRING,\n",
    "  warc_filename                 STRING,\n",
    "  warc_record_offset            INT,\n",
    "  warc_record_length            INT,\n",
    "  warc_segment                  STRING)\n",
    "PARTITIONED BY (\n",
    "  crawl                         STRING,\n",
    "  subset                        STRING)\n",
    "STORED AS parquet\n",
    "LOCATION 's3://commoncrawl/cc-index/table/cc-main/warc/';</code>\n",
    "\n",
    "#### Query 3\n",
    "<code>MSCK REPAIR TABLE ccindex</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the core configurations notebook. This generally only needs to be run once.\n",
    "%run ./configuration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core functions notebook contains generalized functions that apply across use cases\n",
    "%run ./corefunctions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for common crawl from the configuration.ipynb notebook\n",
    "dependencies_commoncrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to update these values\n",
    "DOMAIN_TO_QUERY = 'derbycon.com' # This should look like 'domain.com'. The wildcard will be added automatically later.\n",
    "ATHENA_BUCKET = 's3://brevity-athena' # This will need to be customized and specific to your own account (i.e. s3://customname-athena').\n",
    "ATHENA_DB = 'ccindex' # This should align with the database and not need changed if it was created using the previous queries.\n",
    "ATHENA_TABLE = 'ccindex' # This should align with the table and not need changed if it was created using the previous queries.\n",
    "\n",
    "# Do not modify this query unless the intent is to customize\n",
    "query = \"SELECT url, url_query, warc_filename, warc_record_offset, warc_record_length, fetch_time FROM %s WHERE subset = 'warc' AND url_host_registered_domain = '%s';\" % (ATHENA_TABLE, DOMAIN_TO_QUERY)\n",
    "\n",
    "%time execid = queryathena(ATHENA_DB, ATHENA_BUCKET, query)\n",
    "print(execid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, time, requests\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Load an external notebook with normalized functions\n",
    "%run ./corefunctions.ipynb\n",
    "    \n",
    "# Utilize executionID to retrieve results\n",
    "downloadURL = retrieveresults(execid)\n",
    "\n",
    "# Load output into dataframe\n",
    "s=requests.get(downloadURL).content\n",
    "dfhosts=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "dfhosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates keeping the latest version - if you want to review changes between fetch, you may not want to run this\n",
    "dfhosts = dfhosts.sort_values('fetch_time').drop_duplicates('url',keep='last',ignore_index=True)\n",
    "dfhosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to excel spreadsheet\n",
    "dfhosts['url'].to_excel(\"cc-urls.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "dfhosts['url'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warcio\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4 import Comment\n",
    "import io\n",
    "from io import BytesIO\n",
    "import logging\n",
    "import dask.dataframe as ddf\n",
    "import multiprocessing\n",
    "\n",
    "# Thank you to Sebastian Nagel for your instructions and code to perform the following step.\n",
    "# http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n",
    "titles_list = []\n",
    "uris_list = []\n",
    "links_list = []\n",
    "comments_list = []\n",
    "\n",
    "#Fetch all WARC records defined by filenames and offsets in rows, parse the records and the contained HTML, split the text into words and emit pairs <word, 1>\n",
    "def processwarcrecords(dfhosts, writefiles, searchfiles, howmanyrecords):\n",
    "    s3client = boto3.client('s3')\n",
    "    recordcount = 0\n",
    "    skippedrecords = 0\n",
    "    processedrecords = 0\n",
    "    totalrecords = len(dfhosts.index)\n",
    "    if howmanyrecords == 0:\n",
    "        howmanyrecords = totalrecords\n",
    "    for index, row in dfhosts.iterrows():\n",
    "        if recordcount > howmanyrecords:\n",
    "            break\n",
    "        recordcount = recordcount + 1\n",
    "        print('Processing row ' + str(recordcount) + ' of ' + str(totalrecords) + ' total rows.')\n",
    "        print('Processed ' + str(processedrecords) + ' records.')\n",
    "        url = row['url']\n",
    "        warc_path = row['warc_filename']\n",
    "        offset = int(row['warc_record_offset'])\n",
    "        length = int(row['warc_record_length'])\n",
    "        rangereq = 'bytes={}-{}'.format(offset, (offset+length-1))\n",
    "        response = s3client.get_object(Bucket='commoncrawl',Key=warc_path,Range=rangereq)\n",
    "        record_stream = BytesIO(response[\"Body\"].read())\n",
    "        for record in ArchiveIterator(record_stream):\n",
    "            if record.rec_type == 'response':\n",
    "                try:\n",
    "                    warc_target_uri = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                    page = record.content_stream().read()\n",
    "                    soup = BeautifulSoup(page, 'html.parser') # lxml should be faster but is not\n",
    "                    title = soup.title.string\n",
    "                    titles_list.append((warc_target_uri, title))\n",
    "                    uris_list.append((warc_target_uri))\n",
    "                    if searchfiles == 'yes':\n",
    "                        # Find all links\n",
    "                        for link in soup.find_all('a'):\n",
    "                            links_list.append((warc_target_uri, link.get('href')))\n",
    "                        # Find all links\n",
    "                        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "                            comments_list.append((warc_target_uri, comment))\n",
    "                    if writefiles == 'yes':\n",
    "                        page = page.decode(\"utf-8\") \n",
    "                        url = url.replace(\"https://\",\"\")\n",
    "                        url = url.replace(\"http://\",\"\")\n",
    "                        url = url + str(offset) + '.html'\n",
    "                        filepath = os.getcwd() + '/tmp/' + url\n",
    "                        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "                        with open(filepath, \"w\") as text_file:\n",
    "                            text_file.write(soup.prettify())\n",
    "                            processedrecords = processedrecords + 1\n",
    "                except Exception as e:\n",
    "                    logger = logging.getLogger('errorhandler')\n",
    "                    print(logger.error('Error: '+ str(e)))\n",
    "                    skippedrecords = skippedrecords + 1\n",
    "                    print('Skipped ' + str(skippedrecords) + ' records.')\n",
    "\n",
    "\n",
    "searchfiles = 'yes' # anything other than 'yes' will not process\n",
    "writefiles = 'yes' # anything other than 'yes' will not process\n",
    "howmanyrecords = 0 # 0 is all records; other options would be a numeric value\n",
    "processwarcrecords(dfhosts,writefiles,searchfiles,howmanyrecords)\n",
    "\n",
    "#df_dask = ddf.from_pandas(dfhosts, npartitions=4)   # where the number of partitions is the number of cores you want to use\n",
    "#df_dask.apply(lambda x: processwarcrecords(dfhosts,writefiles,searchfiles,howmanyrecords), axis=1, meta=('str')).compute(scheduler='multiprocessing')\n",
    "#df_dask['output'] = df_dask.apply(lambda x: your_function(x), meta=('str')).compute(scheduler='multiprocessing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lists into dataframes for further processing\n",
    "dfcomments = pd.DataFrame(comments_list,columns=['URI','Comment'])\n",
    "dftitles = pd.DataFrame(titles_list,columns=['URI','Title'])\n",
    "dflinks = pd.DataFrame(links_list,columns=['URI','Link'])\n",
    "#dflinks.head(10)\n",
    "dfcomments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for keywords within the comments\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "search_values = ['password','token','key','pwd','secret','encrypt','debug','internal','confidential','cookie','admin','jwt']\n",
    "dfcomments[dfcomments.Comment.str.contains('|'.join(search_values ),flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export html search results to excel\n",
    "\n",
    "#with pd.ExcelWriter('cc-domains.xlsx') as writer:  \n",
    "#    dfcomments.to_excel(writer, sheet_name='comments')\n",
    "#    dftitles.to_excel(writer, sheet_name='titles')\n",
    "#    dflinks.to_excel(writer, sheet_name='links')\n",
    "\n",
    "# if dataframe has over 65535 rows, Excel will skip data. In this situation, .csv is better.\n",
    "#compression_opts = dict(method='zip',archive_name='out.csv')  \n",
    "dfcomments.to_csv('dfcomments.csv', header=True, index=False)\n",
    "dftitles.to_csv('dftitles.csv', header=True, index=False) \n",
    "dflinks.to_csv('dflinks.csv', header=True, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zip the file for download out of Jupyter\n",
    "filepath = os.getcwd() + '/tmp/'\n",
    "# This will create a file named domainoutput.tar.gz with the full html files in the structure of the website. It can be downloaded from the same directory running the notebook.\n",
    "! tar -zcvf domainoutput.tar.gz $filepath\n",
    "# This will clean-up the tmp folder\n",
    "! rm -rf tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional example queries to run with this configuration.\n",
    "# These can be run directly within the Athena query window in the AWS console or can be integrated into this notebook instead of using the pre-defined query.\n",
    "\n",
    "# Search the entire common crawl data set for specific URL parameters.\n",
    "SELECT url,\n",
    "       warc_filename,\n",
    "       warc_record_offset,\n",
    "       warc_record_length,\n",
    "       url_query\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE subset = 'warc'\n",
    "  AND url_query like 'cmd='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct websites for a specific domain\n",
    "\n",
    "SELECT DISTINCT COUNT(url) as URLCount\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE  subset = 'warc'\n",
    "  AND url_host_registered_domain = 'domain.com'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
