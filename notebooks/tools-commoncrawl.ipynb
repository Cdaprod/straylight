{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be used to query the Common Crawl data set.\n",
    "\n",
    "# http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n",
    "# To-do: Codify the creation of a S3 bucket to save the Athena results\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to utilizing this notebook, the following three queries should be run within AWS Athena to configure the Common Crawl database.\n",
    "# Do not run this cell in the notebook as it will fail.\n",
    "\n",
    "# Query 1\n",
    "CREATE DATABASE ccindex\n",
    "\n",
    "# Query 2\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (\n",
    "  url_surtkey                   STRING,\n",
    "  url                           STRING,\n",
    "  url_host_name                 STRING,\n",
    "  url_host_tld                  STRING,\n",
    "  url_host_2nd_last_part        STRING,\n",
    "  url_host_3rd_last_part        STRING,\n",
    "  url_host_4th_last_part        STRING,\n",
    "  url_host_5th_last_part        STRING,\n",
    "  url_host_registry_suffix      STRING,\n",
    "  url_host_registered_domain    STRING,\n",
    "  url_host_private_suffix       STRING,\n",
    "  url_host_private_domain       STRING,\n",
    "  url_protocol                  STRING,\n",
    "  url_port                      INT,\n",
    "  url_path                      STRING,\n",
    "  url_query                     STRING,\n",
    "  fetch_time                    TIMESTAMP,\n",
    "  fetch_status                  SMALLINT,\n",
    "  content_digest                STRING,\n",
    "  content_mime_type             STRING,\n",
    "  content_mime_detected         STRING,\n",
    "  content_charset               STRING,\n",
    "  content_languages             STRING,\n",
    "  warc_filename                 STRING,\n",
    "  warc_record_offset            INT,\n",
    "  warc_record_length            INT,\n",
    "  warc_segment                  STRING)\n",
    "PARTITIONED BY (\n",
    "  crawl                         STRING,\n",
    "  subset                        STRING)\n",
    "STORED AS parquet\n",
    "LOCATION 's3://commoncrawl/cc-index/table/cc-main/warc/';\n",
    "\n",
    "# Query 3\n",
    "MSCK REPAIR TABLE ccindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core functions notebook contains generalized functions that apply across use cases\n",
    "%run ./corefunctions.ipynb\n",
    "\n",
    "# Make sure to update these values\n",
    "DOMAIN_TO_QUERY = 'derbycon.com' # This should look like 'domain.com'. The wildcard will be added automatically later.\n",
    "ATHENA_BUCKET = 's3://brevity-athena' # This will need to be customized and specific to your own account (i.e. s3://customname-athena').\n",
    "ATHENA_DB = 'ccindex' # This should align with the database and not need changed if it was created using the previous queries.\n",
    "ATHENA_TABLE = 'ccindex' # This should align with the table and not need changed if it was created using the previous queries.\n",
    "\n",
    "# Do not modify this query unless the intent is to customize\n",
    "query = \"SELECT url, warc_filename, warc_record_offset, warc_record_length FROM %s WHERE subset = 'warc' AND url_host_registered_domain = '%s';\" % (ATHENA_TABLE, DOMAIN_TO_QUERY)\n",
    "\n",
    "execid = queryathena(ATHENA_DB, ATHENA_BUCKET, query)\n",
    "print(execid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, time, requests\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Load an external notebook with normalized functions\n",
    "%run ./corefunctions.ipynb\n",
    "    \n",
    "# Utilize executionID to retrieve results\n",
    "downloadURL = retrieveresults(execid)\n",
    "\n",
    "# Load output into dataframe - re-run the following section of code if the initial process fails.\n",
    "s=requests.get(downloadURL).content\n",
    "dfhosts=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "dfhosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warcio\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "# Thank you to Sebastian Nagel for your instructions and code to perform the following step.\n",
    "# http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n",
    "\n",
    "#Fetch all WARC records defined by filenames and offsets in rows, parse the records and the contained HTML, split the text into words and emit pairs <word, 1>\n",
    "def fetch_process_warc_records(dfhosts):\n",
    "    s3client = boto3.client('s3')\n",
    "    skippedrecords = 0\n",
    "    processedrecords = 0\n",
    "    for index, row in dfhosts.iterrows():\n",
    "        url = row['url']\n",
    "        warc_path = row['warc_filename']\n",
    "        offset = int(row['warc_record_offset'])\n",
    "        length = int(row['warc_record_length'])\n",
    "        rangereq = 'bytes={}-{}'.format(offset, (offset+length-1))\n",
    "        response = s3client.get_object(Bucket='commoncrawl',Key=warc_path,Range=rangereq)\n",
    "        record_stream = BytesIO(response[\"Body\"].read())\n",
    "        for record in ArchiveIterator(record_stream):\n",
    "            try:\n",
    "                page = record.content_stream().read()\n",
    "                page = page.decode(\"utf-8\") \n",
    "                url = url.replace(\"https://\",\"\")\n",
    "                url = url.replace(\"http://\",\"\")\n",
    "                url = url + str(offset) + '.html'\n",
    "                filepath = os.getcwd() + '/tmp/' + url\n",
    "                os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "                with open(filepath, \"w\") as text_file:\n",
    "                    print(page, file=text_file)\n",
    "                    processedrecords = processedrecords + 1\n",
    "            except:\n",
    "                # Proceed without printing the record. Occasionally there are decoding errors.\n",
    "                skippedrecords = skippedrecords + 1\n",
    "\n",
    "# Nothing is actually returned from this command as it writes the files to the filesystem.\n",
    "#Todo: Add some type of return metric.\n",
    "dfhostsupdated = fetch_process_warc_records(dfhosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zip the file for download out of Jupyter\n",
    "filepath = os.getcwd() + '/tmp/'\n",
    "! tar -zcvf domainoutput.tar.gz $filepath\n",
    "# This will create a file named domainoutput.tar.gz with the full html files in the structure of the website. It can be downloaded from the same directory running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional example queries to run with this configuration.\n",
    "# These can be run directly within the Athena query window in the AWS console or can be integrated into this notebook instead of using the pre-defined query.\n",
    "\n",
    "# Search the entire common crawl data set for specific URL parameters.\n",
    "SELECT url,\n",
    "       warc_filename,\n",
    "       warc_record_offset,\n",
    "       warc_record_length,\n",
    "       url_query\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE subset = 'warc'\n",
    "  AND url_query like 'cmd='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct websites for a specific domain\n",
    "\n",
    "SELECT DISTINCT COUNT(url) as URLCount\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE  subset = 'warc'\n",
    "  AND url_host_registered_domain = 'domain.com'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
