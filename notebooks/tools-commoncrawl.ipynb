{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be used to query the Common Crawl data set.\n",
    "\n",
    "# http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n",
    "# To-do: Codify the creation of a S3 bucket to save the Athena results\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to utilizing this notebook, the following three queries should be run within AWS Athena to configure the Common Crawl database.\n",
    "# Do not run this cell in the notebook as it will fail.\n",
    "\n",
    "# Query 1\n",
    "CREATE DATABASE ccindex\n",
    "\n",
    "# Query 2\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (\n",
    "  url_surtkey                   STRING,\n",
    "  url                           STRING,\n",
    "  url_host_name                 STRING,\n",
    "  url_host_tld                  STRING,\n",
    "  url_host_2nd_last_part        STRING,\n",
    "  url_host_3rd_last_part        STRING,\n",
    "  url_host_4th_last_part        STRING,\n",
    "  url_host_5th_last_part        STRING,\n",
    "  url_host_registry_suffix      STRING,\n",
    "  url_host_registered_domain    STRING,\n",
    "  url_host_private_suffix       STRING,\n",
    "  url_host_private_domain       STRING,\n",
    "  url_protocol                  STRING,\n",
    "  url_port                      INT,\n",
    "  url_path                      STRING,\n",
    "  url_query                     STRING,\n",
    "  fetch_time                    TIMESTAMP,\n",
    "  fetch_status                  SMALLINT,\n",
    "  content_digest                STRING,\n",
    "  content_mime_type             STRING,\n",
    "  content_mime_detected         STRING,\n",
    "  content_charset               STRING,\n",
    "  content_languages             STRING,\n",
    "  warc_filename                 STRING,\n",
    "  warc_record_offset            INT,\n",
    "  warc_record_length            INT,\n",
    "  warc_segment                  STRING)\n",
    "PARTITIONED BY (\n",
    "  crawl                         STRING,\n",
    "  subset                        STRING)\n",
    "STORED AS parquet\n",
    "LOCATION 's3://commoncrawl/cc-index/table/cc-main/warc/';\n",
    "\n",
    "# Query 3\n",
    "MSCK REPAIR TABLE ccindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core functions notebook contains generalized functions that apply across use cases\n",
    "%run ./corefunctions.ipynb\n",
    "\n",
    "# Make sure to update these values\n",
    "DOMAIN_TO_QUERY = 'derbycon.com' # This should look like 'domain.com'. The wildcard will be added automatically later.\n",
    "ATHENA_BUCKET = 's3://brevity-athena' # This will need to be customized and specific to your own account (i.e. s3://customname-athena').\n",
    "ATHENA_DB = 'ccindex' # This should align with the database and not need changed if it was created using the previous queries.\n",
    "ATHENA_TABLE = 'ccindex' # This should align with the table and not need changed if it was created using the previous queries.\n",
    "\n",
    "# Do not modify this query unless the intent is to customize\n",
    "query = \"SELECT url, url_query, warc_filename, warc_record_offset, warc_record_length FROM %s WHERE subset = 'warc' AND url_host_registered_domain = '%s';\" % (ATHENA_TABLE, DOMAIN_TO_QUERY)\n",
    "\n",
    "execid = queryathena(ATHENA_DB, ATHENA_BUCKET, query)\n",
    "print(execid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, time, requests\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Load an external notebook with normalized functions\n",
    "%run ./corefunctions.ipynb\n",
    "    \n",
    "# Utilize executionID to retrieve results\n",
    "downloadURL = retrieveresults(execid)\n",
    "\n",
    "# Load output into dataframe\n",
    "s=requests.get(downloadURL).content\n",
    "dfhosts=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "dfhosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warcio\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import os\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4 import Comment\n",
    "import io\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "# Thank you to Sebastian Nagel for your instructions and code to perform the following step.\n",
    "# http://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf\n",
    "titles_list = []\n",
    "uris_list = []\n",
    "links_list = []\n",
    "comments_list = []\n",
    "\n",
    "#Fetch all WARC records defined by filenames and offsets in rows, parse the records and the contained HTML, split the text into words and emit pairs <word, 1>\n",
    "def processwarcrecords(dfhosts, writefiles, searchfiles, howmanyrecords):\n",
    "    s3client = boto3.client('s3')\n",
    "    recordcount = 0\n",
    "    skippedrecords = 0\n",
    "    processedrecords = 0\n",
    "    totalrecords = len(dfhosts.index)\n",
    "    if howmanyrecords == 0:\n",
    "        howmanyrecords = totalrecords\n",
    "    for index, row in dfhosts.iterrows():\n",
    "        if recordcount > howmanyrecords:\n",
    "            break\n",
    "        recordcount = recordcount + 1\n",
    "        print('Processing row ' + str(recordcount) + ' of ' + str(totalrecords) + ' total rows.')\n",
    "        print('Processed ' + str(processedrecords) + ' records.')\n",
    "        url = row['url']\n",
    "        warc_path = row['warc_filename']\n",
    "        offset = int(row['warc_record_offset'])\n",
    "        length = int(row['warc_record_length'])\n",
    "        rangereq = 'bytes={}-{}'.format(offset, (offset+length-1))\n",
    "        response = s3client.get_object(Bucket='commoncrawl',Key=warc_path,Range=rangereq)\n",
    "        record_stream = BytesIO(response[\"Body\"].read())\n",
    "        for record in ArchiveIterator(record_stream):\n",
    "            if record.rec_type == 'response':\n",
    "                try:\n",
    "                    warc_target_uri = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                    page = record.content_stream().read()\n",
    "                    soup = BeautifulSoup(page, 'html.parser') # lxml should be faster but is not\n",
    "                    title = soup.title.string\n",
    "                    titles_list.append((warc_target_uri, title))\n",
    "                    uris_list.append((warc_target_uri))\n",
    "                    if searchfiles == 'yes':\n",
    "                        # Find all links\n",
    "                        for link in soup.find_all('a'):\n",
    "                            links_list.append((warc_target_uri, link.get('href')))\n",
    "                        # Find all links\n",
    "                        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "                            comments_list.append((warc_target_uri, comment))\n",
    "                    if writefiles == 'yes':\n",
    "                        page = page.decode(\"utf-8\") \n",
    "                        url = url.replace(\"https://\",\"\")\n",
    "                        url = url.replace(\"http://\",\"\")\n",
    "                        url = url + str(offset) + '.html'\n",
    "                        filepath = os.getcwd() + '/tmp/' + url\n",
    "                        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "                        with open(filepath, \"w\") as text_file:\n",
    "                            text_file.write(soup.prettify())\n",
    "                            processedrecords = processedrecords + 1\n",
    "                except Exception as e:\n",
    "                    logger = logging.getLogger('errorhandler')\n",
    "                    print(logger.error('Error: '+ str(e)))\n",
    "                    skippedrecords = skippedrecords + 1\n",
    "                    print('Skipped ' + str(skippedrecords) + ' records.')\n",
    "\n",
    "\n",
    "searchfiles = 'yes' # anything other than 'yes' will not process\n",
    "writefiles = 'yes' # anything other than 'yes' will not process\n",
    "howmanyrecords = 0 # 0 is all records; other options would be a numeric value\n",
    "processwarcrecords(dfhosts,writefiles,searchfiles,howmanyrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to show outputs\n",
    "#uris_list[1:10]\n",
    "#titles_list[1:10]\n",
    "#comments_list[1:10]\n",
    "#links_list[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zip the file for download out of Jupyter\n",
    "filepath = os.getcwd() + '/tmp/'\n",
    "# This will create a file named domainoutput.tar.gz with the full html files in the structure of the website. It can be downloaded from the same directory running the notebook.\n",
    "! tar -zcvf domainoutput.tar.gz $filepath\n",
    "# This will clean-up the tmp folder\n",
    "! rm -rf tmp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional example queries to run with this configuration.\n",
    "# These can be run directly within the Athena query window in the AWS console or can be integrated into this notebook instead of using the pre-defined query.\n",
    "\n",
    "# Search the entire common crawl data set for specific URL parameters.\n",
    "SELECT url,\n",
    "       warc_filename,\n",
    "       warc_record_offset,\n",
    "       warc_record_length,\n",
    "       url_query\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE subset = 'warc'\n",
    "  AND url_query like 'cmd='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct websites for a specific domain\n",
    "\n",
    "SELECT DISTINCT COUNT(url) as URLCount\n",
    "FROM \"ccindex\".\"ccindex\"\n",
    "WHERE  subset = 'warc'\n",
    "  AND url_host_registered_domain = 'domain.com'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
